{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Indentation: Jupyter Notebook\n",
    "\n",
    "'''\n",
    "Clustering using SparkML\n",
    "'''\n",
    "\n",
    "__version__ = 1.0\n",
    "__author__ = \"Sourav Raj\"\n",
    "__author_email__ = \"souravraj.iitbbs@gmail.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc =SparkContext()\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = spark.read.csv('../data/clustering_dataset.csv', header=True, inferSchema =True)\n",
    "# inferSchema =True to read numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=7, col2=4, col3=1),\n",
       " Row(col1=7, col2=7, col3=9),\n",
       " Row(col1=7, col2=9, col3=6)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using vector assembler we convert these columns to features\n",
    "vec_assembler=VectorAssembler(inputCols=['col1', 'col2', 'col3'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=7, col2=4, col3=1, features=DenseVector([7.0, 4.0, 1.0])),\n",
       " Row(col1=7, col2=7, col3=9, features=DenseVector([7.0, 7.0, 9.0])),\n",
       " Row(col1=7, col2=9, col3=6, features=DenseVector([7.0, 9.0, 6.0])),\n",
       " Row(col1=1, col2=6, col3=5, features=DenseVector([1.0, 6.0, 5.0])),\n",
       " Row(col1=6, col2=7, col3=7, features=DenseVector([6.0, 7.0, 7.0]))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df = vec_assembler.transform(cluster_df)\n",
    "vec_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans=KMeans().setK(3) # set no of cluster k =3\n",
    "kmeans=kmeans.setSeed(1) # setseed to set where kmean cluster will start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodel=kmeans.fit(vec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmodel.clusterCenters()  # to get center of cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 35.88461538,  31.46153846,  34.42307692]),\n",
       " array([ 5.12,  5.84,  4.84]),\n",
       " array([ 80.        ,  79.20833333,  78.29166667])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for large data we use this algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "bkmeans=BisectingKMeans().setK(3)\n",
    "bkmeans=bkmeans.setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkmodel=bkmeans.fit(vec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 5.12,  5.84,  4.84]),\n",
       " array([ 35.88461538,  31.46153846,  34.42307692]),\n",
       " array([ 80.        ,  79.20833333,  78.29166667])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bkcenters=bkmodel.clusterCenters()\n",
    "bkcenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
