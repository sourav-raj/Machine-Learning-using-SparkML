{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Indentation: Jupyter Notebook\n",
    "\n",
    "'''\n",
    "Data Preprocessing using Spark\n",
    "'''\n",
    "\n",
    "__version__ = 1.0\n",
    "__author__ = \"Sourav Raj\"\n",
    "__author_email__ = \"souravraj.iitbbs@gmail.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc =SparkContext()\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df=spark.createDataFrame([\n",
    "    (1, Vectors.dense([10.0, 10000.0, 1.0]),),\n",
    "    (2, Vectors.dense([20.0, 20000.0, 2.0]),),\n",
    "    (3, Vectors.dense([20.0, 30000.0, 5.0]),),\n",
    "    (4, Vectors.dense([30.0, 40000.0, 10.0]),)],['id', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df=spark.createDataFrame([\n",
    "    (1, Vectors.dense([10.0, 10000.0]),),\n",
    "    (2, Vectors.dense([15.0, 20000.0]),),\n",
    "    (3, Vectors.dense([20.0, 30000.0]),),\n",
    "    (4, Vectors.dense([30.0, 40000.0]),)],['id', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a scalar object\n",
    "feature_scaler = MinMaxScaler(inputCol='features', outputCol='scaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel=feature_scaler.fit(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfeatures_df= smodel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), scaled_features=DenseVector([0.0, 0.0, 0.0]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfeatures_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|      features|     scaled_features|\n",
      "+--------------+--------------------+\n",
      "|[10.0,10000.0]|           [0.0,0.0]|\n",
      "|[15.0,20000.0]|[0.25,0.333333333...|\n",
      "|[20.0,30000.0]|[0.5,0.6666666666...|\n",
      "|[30.0,40000.0]|           [1.0,1.0]|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfeatures_df.select('features', 'scaled_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|features|scaled_features|\n",
      "+--------+---------------+\n",
      "|  [10.0]|          [0.0]|\n",
      "|  [15.0]|         [0.25]|\n",
      "|  [20.0]|          [0.5]|\n",
      "|  [30.0]|          [1.0]|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfeatures_df.select('features', 'scaled_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|           features|     scaled_features|\n",
      "+-------------------+--------------------+\n",
      "| [10.0,10000.0,1.0]|       [0.0,0.0,0.0]|\n",
      "| [20.0,20000.0,2.0]|[0.5,0.3333333333...|\n",
      "|[30.0,40000.0,10.0]|       [1.0,1.0,1.0]|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfeatures_df.select('features', 'scaled_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|          features|     scaled_features|\n",
      "+------------------+--------------------+\n",
      "|[10.0,10000.0,1.0]|       [0.0,0.0,0.0]|\n",
      "|[20.0,20000.0,2.0]|[0.5,0.3333333333...|\n",
      "|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfeatures_df.select('features', 'scaled_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|          features|     scaled_features|\n",
      "+------------------+--------------------+\n",
      "|[10.0,10000.0,1.0]|       [0.0,0.0,0.0]|\n",
      "|[20.0,30000.0,2.0]|[0.5,0.6666666666...|\n",
      "|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfeatures_df.select('features', 'scaled_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__del__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__metaclass__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_call_java', '_clear', '_copyValues', '_copy_params', '_create_from_java_class', '_create_model', '_create_params_from_java', '_dummy', '_empty_java_param_map', '_fit', '_fit_java', '_from_java', '_make_java_param_pair', '_new_java_array', '_new_java_obj', '_randomUID', '_resetUid', '_resolveParam', '_set', '_setDefault', '_shouldOwn', '_to_java', '_transfer_param_map_from_java', '_transfer_param_map_to_java', '_transfer_params_from_java', '_transfer_params_to_java', 'copy', 'explainParam', 'explainParams', 'extractParamMap', 'fit', 'fitMultiple', 'getInputCol', 'getMax', 'getMin', 'getOrDefault', 'getOutputCol', 'getParam', 'hasDefault', 'hasParam', 'inputCol', 'isDefined', 'isSet', 'load', 'max', 'min', 'outputCol', 'params', 'read', 'save', 'set', 'setInputCol', 'setMax', 'setMin', 'setOutputCol', 'setParams', 'write']\n"
     ]
    }
   ],
   "source": [
    "print(dir(MinMaxScaler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df=spark.createDataFrame([\n",
    "    (1, Vectors.dense([10.0, 10000.0, 1.0]),),\n",
    "    (2, Vectors.dense([20.0, 20000.0, 2.0]),),\n",
    "    (3, Vectors.dense([20.0, 30000.0, 5.0]),),\n",
    "    (4, Vectors.dense([30.0, 40000.0, 10.0]),)],['id', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stand_scaler=StandardScaler(inputCol='features', outputCol='sfeatures', withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_smodel=feature_stand_scaler.fit(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_sfeatures_df=stand_smodel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sfeatures=DenseVector([-1.2247, -1.1619, -0.866]))]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand_sfeatures_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "| id|           features|           sfeatures|\n",
      "+---+-------------------+--------------------+\n",
      "|  1| [10.0,10000.0,1.0]|[-1.2247448713915...|\n",
      "|  2| [20.0,20000.0,2.0]|[0.0,-0.387298334...|\n",
      "|  3| [20.0,30000.0,5.0]|[0.0,0.3872983346...|\n",
      "|  4|[30.0,40000.0,10.0]|[1.22474487139158...|\n",
      "+---+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stand_sfeatures_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketize numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "# bucketizer work with range of boundaries \n",
    "splits=[-float('inf'), -10.0, 0.0, 10.0, float('inf') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data = [(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df=spark.createDataFrame(b_data, ['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|  -800.0|\n",
      "|   -10.5|\n",
      "|    -1.7|\n",
      "|     0.0|\n",
      "|     8.2|\n",
      "|    90.1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketizer = Bucketizer(splits=splits, inputCol='features', outputCol='bfeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|features|bfeatures|\n",
      "+--------+---------+\n",
      "|  -800.0|      0.0|\n",
      "|   -10.5|      0.0|\n",
      "|    -1.7|      1.0|\n",
      "|     0.0|      2.0|\n",
      "|     8.2|      2.0|\n",
      "|    90.1|      3.0|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketed_df=bucketizer.transform(b_df)\n",
    "bucketed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df=spark.createDataFrame([(1, 'This is an introduction to Spark MLlib'),\n",
    "                                   (2, 'MLlib includes libraries for classification'),\n",
    "                                   (3, 'It also contains supporting tools for pipelines')],\n",
    "                                  ['id', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  1|This is an introd...|\n",
      "|  2|MLlib includes li...|\n",
      "|  3|It also contains ...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token= Tokenizer(inputCol='sentence', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenized_df = sent_token.transform(sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|\n",
      "|  2|MLlib includes li...|[mllib, includes,...|\n",
      "|  3|It also contains ...|[it, also, contai...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_tokenized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashigTF=HashingTF(inputCol='words', outputCol='rawfeatures', numFeatures=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence=u'This is an introduction to Spark MLlib', words=[u'this', u'is', u'an', u'introduction', u'to', u'spark', u'mllib'], rawfeatures=SparseVector(20, {1: 2.0, 5: 1.0, 6: 1.0, 8: 1.0, 12: 1.0, 13: 1.0}))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_hfTF_df=hashigTF.transform(sent_tokenized_df)\n",
    "sent_hfTF_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf =IDF(inputCol='rawfeatures', outputCol='idf_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel=idf.fit(sent_hfTF_df)\n",
    "tfidf_df=idfModel.transform(sent_hfTF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence=u'This is an introduction to Spark MLlib', words=[u'this', u'is', u'an', u'introduction', u'to', u'spark', u'mllib'], rawfeatures=SparseVector(20, {1: 2.0, 5: 1.0, 6: 1.0, 8: 1.0, 12: 1.0, 13: 1.0}), idf_features=SparseVector(20, {1: 0.5754, 5: 0.6931, 6: 0.2877, 8: 0.2877, 12: 0.0, 13: 0.6931}))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
